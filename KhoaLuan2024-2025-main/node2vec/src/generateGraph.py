import numpy as np
from scipy import spatial
import networkx as nx
from gensim.models import KeyedVectors
import gensim.downloader as api
dim = 150  

# HÃ m láº¥y vector tá»« model
def getVectors(u1):
    u1 = u1.replace("-", "_")
    vs = np.repeat(0, dim)
    mk = 1
    #if u1 in model.key_to_index:
    if u1 in model.keys():
        vs = model[u1]
    else:
        uu = u1.split("_")
        for i in uu:
            #if u1 in model.key_to_index:
            if i in model.keys():
                if mk == 1:
                    vs = model[i]
                    mk = 0  
                else:
                    vs = vs + model[i]
    return np.array(vs)
# Load model tá»« file
'''
try:
    model = KeyedVectors.load_word2vec_format("word/fastText_4GB.vec", binary=True)
    dim = model.vector_size
    print("âœ… MÃ´ hÃ¬nh Word2Vec Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng.\n")
except UnicodeDecodeError:
    print("âš ï¸ File cÃ³ thá»ƒ khÃ´ng pháº£i nhá»‹ phÃ¢n, thá»­ láº¡i vá»›i binary=False...")
    try:
        model = KeyedVectors.load_word2vec_format("word/fastText_4GB.vec", binary=False)
        dim = model.vector_size
        print("âœ… MÃ´ hÃ¬nh Word2Vec Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng (dáº¡ng vÄƒn báº£n).\n")
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ táº£i mÃ´ hÃ¬nh: {e}")
        exit()
'''
fWord2vec = 'word/W2V_150.txt'
model = {}
try:
    with open(fWord2vec, 'r', encoding="utf-8-sig") as f:
        f.readline()  # Bá» qua dÃ²ng Ä‘áº§u tiÃªn

        words_list1 = []  # Danh sÃ¡ch chá»©a cÃ¡c tá»« Ä‘Ã£ Ä‘á»c

        for line in f:
            tem = line.split()

            if len(tem) != dim+1:  # 1 tá»« + 100 giÃ¡ trá»‹ vector
                continue  

            word = tem[0]  # Láº¥y tá»«
            try:
                vector = np.array(list(map(float, tem[1:])))  # Chuyá»ƒn Ä‘á»•i vector
                model[word] = vector  # ThÃªm vÃ o dictionary
                words_list1.append(word)  # LÆ°u tá»« vÃ o danh sÃ¡ch
            except ValueError:
                print(f"âš ï¸ Bá» qua tá»« '{word}' do lá»—i chuyá»ƒn Ä‘á»•i sá»‘.")

    print(f"\nâœ… Äá»c xong {len(model)} tá»« vá»›i vector kÃ­ch thÆ°á»›c {len(vector)}.")

    # In ra danh sÃ¡ch cÃ¡c tá»« Ä‘Ã£ Ä‘á»c
    print("\nğŸ“Œ CÃ¡c tá»« Ä‘Ã£ Ä‘á»c Ä‘Æ°á»£c:")
    print(len(words_list1))  # In 50 tá»« Ä‘áº§u tiÃªn Ä‘á»ƒ kiá»ƒm tra

except FileNotFoundError:
    print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{fWord2vec}'. Kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.")
except Exception as e:
    print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
word_txt = "word/VSimlex.txt"
word_list=[]

with open(word_txt, "r", encoding="utf-8") as file:
    #next(file)  
    for line in file:
        parts = line.strip().split("\t")  
        if len(parts) >= 2:
            word1 = parts[0].replace(' ', '_')  
            word2 = parts[1].replace(' ', '_')
            word_list.append(word1)  
            word_list.append(word2)
word_list = list(set(word_list))
G = nx.Graph()

for i in range(len(word_list)-1):
    w1 = word_list[i]
    v1 = getVectors(w1)  
    
    for j in range(i + 1, len(word_list)):
        w2 = word_list[j]
        v2 = getVectors(w2)  
        
        if np.all(v1 == 0) or np.all(v2 == 0):  
            continue
        k =1.0-spatial.distance.cosine(v1, v2) 
        #k =spatial.distance.cosine(v1, v2) 
        G.add_edge(w1, w2, weight=k)  
edges_to_remove = [(u, v) for u, v, data in G.edges(data=True) if data["weight"] <0.2]
#edges_to_remove = [(u, v) for u, v, data in G.edges(data=True) if data["weight"] >0.9]
G.remove_edges_from(edges_to_remove)
file_path = 'word/Verbs_dn.txt'  # Thay báº±ng Ä‘Æ°á»ng dáº«n tá»›i file cá»§a báº¡n
# Má»Ÿ vÃ  Ä‘á»c dá»¯ liá»‡u tá»« file
with open(file_path, 'r', encoding='utf-8') as file:
    lines = file.readlines()
for line_number, line in enumerate(lines, start=1):
    words = [item.strip().replace(' ', '_') for item in line.strip().split(',')]
    if(len(words)>=2):
        for i in range(0,len(words)-1):
            w1=words[i]
            for j in range(i+1,len(words)):
                w2=words[j]
                if w1!=w2:
                    if not G.has_edge(w1,w2):
                        G.add_edge(w1,w2,weight=0.99)
                    else:
                        data=G.get_edge_data(w1,w2)
                        if(data['weight']<0.99):
                            G[w1][w2]['weight']=0.99
def read_word(file_path):
    word_pairs = []
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                if '-' in line:
                    w1, w2 = line.strip().split('-')
                    w1 = w1.strip().replace(' ', '_')
                    w2 = w2.strip().replace(' ', '_')
                    if w1!=w2:
                        word_pairs.append((w1, w2))
    except FileNotFoundError:
        print(f"KhÃ´ng tÃ¬m tháº¥y file táº¡i Ä‘Æ°á»ng dáº«n: {file_path}")
    
    return word_pairs


word_pairs=read_word('word/dongnghia.txt')
def read_word_trainghia(file_path):
    word_pairs = []
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                parts = line.strip().split()  # TÃ¡ch báº±ng dáº¥u cÃ¡ch (space)
                if len(parts) == 2:  # Chá»‰ xá»­ lÃ½ dÃ²ng cÃ³ Ä‘Ãºng 2 tá»«
                    w1, w2 = parts
                    w1 = w1.replace(' ', '_')  # Äáº£m báº£o Ä‘á»‹nh dáº¡ng tá»«
                    w2 = w2.replace(' ', '_')
                    word_pairs.append((w1, w2))
                else:
                    print(f"âš ï¸ Cáº£nh bÃ¡o: Bá» qua dÃ²ng khÃ´ng há»£p lá»‡ -> {line.strip()}")
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file táº¡i: {file_path}")
    
    return word_pairs

if word_pairs:
    print("CÃ¡c cáº·p tá»« Ä‘Ã£ Ä‘á»c tá»« file:")
    for w1, w2 in word_pairs:
        if w1!=w2 and not G.has_edge(w1,w2):
            G.add_edge(w1,w2,weight=0.99)
else:
    print("KhÃ´ng cÃ³ cáº·p tá»« nÃ o Ä‘Æ°á»£c Ä‘á»c tá»« file.")
word_pairs2=read_word_trainghia('word/trainghia.txt')
if word_pairs2:
    print("CÃ¡c cáº·p tá»« Ä‘Ã£ Ä‘á»c tá»« file:")
    for w1, w2 in word_pairs2:
        if w1!=w2 and not G.has_edge(w1,w2):
            G.add_edge(w1,w2,weight=0.01)
else:
    print("KhÃ´ng cÃ³ cáº·p tá»« nÃ o Ä‘Æ°á»£c Ä‘á»c tá»« file.")
graph_txt = "graph/Noun_Graph.txt"
with open(graph_txt, "w", encoding="utf-8") as f:
    for u, v, data in G.edges(data=True):
        f.write(f"{u} {v} {data['weight']:.2f}\n")
