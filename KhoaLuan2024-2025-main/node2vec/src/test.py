import numpy as np
from scipy.spatial.distance import cosine
from gensim.models import KeyedVectors
from scipy import spatial
from pyvi import ViTokenizer
dim=150
'''
# Táº£i mÃ´ hÃ¬nh word embeddings
try:
    model = KeyedVectors.load("emb/Noun.emb.wv")
    dim = model.vector_size  
    print("âœ… MÃ´ hÃ¬nh Node2Vec Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng!\n")
except Exception as e:
    print(f"âŒ KhÃ´ng thá»ƒ táº£i mÃ´ hÃ¬nh: {e}")
    exit()
'''
fWord2vec = 'word/W2V_150.txt'
model = {}
try:
    with open(fWord2vec, 'r', encoding="utf-8-sig") as f:
        f.readline()  # Bá» qua dÃ²ng Ä‘áº§u tiÃªn

        words_list = []  # Danh sÃ¡ch chá»©a cÃ¡c tá»« Ä‘Ã£ Ä‘á»c

        for line in f:
            tem = line.split()

            if len(tem) != dim+1:  # 1 tá»« + 100 giÃ¡ trá»‹ vector
                continue  

            word = tem[0]  # Láº¥y tá»«
            try:
                vector = np.array(list(map(float, tem[1:])))  # Chuyá»ƒn Ä‘á»•i vector
                model[word] = vector  # ThÃªm vÃ o dictionary
                words_list.append(word)  # LÆ°u tá»« vÃ o danh sÃ¡ch
            except ValueError:
                print(f"âš ï¸ Bá» qua tá»« '{word}' do lá»—i chuyá»ƒn Ä‘á»•i sá»‘.")

    print(f"\nâœ… Äá»c xong {len(model)} tá»« vá»›i vector kÃ­ch thÆ°á»›c {len(vector)}.")

    # In ra danh sÃ¡ch cÃ¡c tá»« Ä‘Ã£ Ä‘á»c
    print("\nğŸ“Œ CÃ¡c tá»« Ä‘Ã£ Ä‘á»c Ä‘Æ°á»£c:")
    print(len(words_list))  # In 50 tá»« Ä‘áº§u tiÃªn Ä‘á»ƒ kiá»ƒm tra

except FileNotFoundError:
    print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{fWord2vec}'. Kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.")
except Exception as e:
    print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
# Danh sÃ¡ch cÃ¡c tá»« phá»§ Ä‘á»‹nh cáº§n xá»­ lÃ½

# HÃ m láº¥y vector tá»« mÃ´ hÃ¬nh
def getVectors(word):
    word = word.replace("-", "_")
    vector = np.zeros(dim)  
    #if word in model.key_to_index:
    if word in model:
        vector = model[word].copy()  
    else:
        parts = word.split("_")
        for i, part in enumerate(parts):
            #if part in model.key_to_index:
            if part in model:
                vector = model[part].copy() if i == 0 else vector + model[part].copy()
    return np.array(vector) 

# Kiá»ƒm tra xem cÃ¢u cÃ³ tá»« phá»§ Ä‘á»‹nh khÃ´ng
def is_antonym(word1, word2, threshold=-0.5):
    v1 = getVectors(word1)
    v2 = getVectors(word2)

    if np.all(v1 == 0) or np.all(v2 == 0):  
        return False  

    similarity = 1 - spatial.distance.cosine(v1, v2)

    return similarity < threshold  

def sentence_similarity_with_antonyms(text1, text2, position_weight=0.3, antonym_penalty=0.5):
    words1 = text1.split()
    words2 = text2.split()
    similarities = []
    position_penalty = 0  
    antonym_count = 0  

    # TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cho tá»«ng tá»« trong cÃ¢u 1 so vá»›i cÃ¢u 2
    for i, word1 in enumerate(words1):
        v1 = getVectors(word1)
        if np.all(v1 == 0):  # Bá» qua tá»« khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn
            continue

        best_match = 0  # Khá»Ÿi táº¡o best_match lÃ  0 thay vÃ¬ -1
        best_pos_diff = len(words1)  
        is_opposite = False  

        for j, word2 in enumerate(words2):
            v2 = getVectors(word2)
            if np.all(v2 == 0):  
                continue

            similarity = 1 - spatial.distance.cosine(v1, v2)

            if similarity > best_match:
                best_match = similarity
                best_pos_diff = abs(i - j)

            # Kiá»ƒm tra tá»« trÃ¡i nghÄ©a
            if is_antonym(word1, word2):
                is_opposite = True
                antonym_count += 1  

        similarities.append(best_match)

        position_penalty += best_pos_diff * position_weight  

        if is_opposite:
            similarities[-1] -= antonym_penalty  

    if not similarities:
        return 0, 0  

    position_penalty = position_penalty / len(words1)

    final_score = np.mean(similarities) - position_penalty
    return max(0, final_score), antonym_count  
def merge_tokens(text):
    words = text.split()  # TÃ¡ch thÃ nh danh sÃ¡ch tá»«
    merged_words = []
    i = 0
    while i < len(words):
        if words[i] in ["khÃ´ng", "chÆ°a", "Ä‘Ã£", "sáº½", "vá»«a", "má»›i"] and i < len(words) - 1:
            merged_words.append(words[i] + "_" + words[i + 1])  # Gá»™p vá»›i tá»« sau
            i += 2  
        else:
            merged_words.append(words[i])
            i += 1
    return " ".join(merged_words)
# Test vá»›i hai cÃ¢u vÃ­ dá»¥
text1 = "nhÃ  thÆ¡ khÃ´ng thÃ­ch thi sÄ©"
text2 = "thi sÄ© ghÃ©t tháº§y thuá»‘c"
text1=ViTokenizer.tokenize(text1)
text2=ViTokenizer.tokenize(text2)
text1 = merge_tokens(text1)
text2 = merge_tokens(text2)
score, antonym_count = sentence_similarity_with_antonyms(text1, text2)
print(f"Similarity Score (with antonyms): {score:.4f}")
print(f"Number of Antonymous Words: {antonym_count}")