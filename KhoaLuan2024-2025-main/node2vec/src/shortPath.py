import networkx as nx
import numpy as np
from gensim.models import KeyedVectors
from scipy.stats import pearsonr
from scipy.spatial.distance import euclidean

# Load pre-trained embeddings
embedding_path = "emb/Noun.emb.wv"
model = KeyedVectors.load(embedding_path)

# Load graph tá»« file cÃ³ sáºµn (Ä‘Ã£ cÃ³ trá»ng sá»‘ lÃ  distance)
G = nx.Graph()
file_path = "graph/New_Graph.txt"

with open(file_path, "r", encoding="utf-8") as file:
    for line in file:
        parts = line.strip().split()
        if len(parts) == 3:
            word1, word2, dist = parts[0], parts[1], float(parts[2])  # Distance Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh sáºµn
            if word1 in model.key_to_index and word2 in model.key_to_index:
                    G.add_edge(word1, word2, weight=dist)  



from scipy.spatial.distance import cosine


def heuristic(node1, node2):
    if node1 in model.key_to_index and node2 in model.key_to_index:
        v1, v2 = model[node1], model[node2]
        cos_dist = 1 - np.dot(v1, v2)
        euc_dist = np.linalg.norm(v1 - v2)
        return 0.5 * cos_dist + 0.5 * euc_dist  # Káº¿t há»£p cáº£ hai metric
    return float("inf")




# ğŸ” **HÃ m tÃ¬m khoáº£ng cÃ¡ch gáº§n nháº¥t tá»« w1 Ä‘áº¿n w2 báº±ng A***
def find_similarity(w1, w2):
    if w1 in G.nodes() and w2 in G.nodes() :
        try:
            cost = nx.astar_path_length(G, w1, w2, heuristic=heuristic, weight="weight")
            #cost = nx.dijkstra_path_length(G, w1, w2, weight="weight")
            #sim_score = np.exp(-cost)  # Chuyá»ƒn distance vá» khoáº£ng [0,1]
            return cost
        except nx.NetworkXNoPath:
            print(f"âŒ KhÃ´ng cÃ³ Ä‘Æ°á»ng Ä‘i tá»« {w1} Ä‘áº¿n {w2}.")
            return None
    else:
        print(f"âš ï¸ Bá» qua {w1} - {w2}: KhÃ´ng cÃ³ trong Ä‘á»“ thá»‹.")
        return None

# ğŸ“Œ **TÃ­nh há»‡ sá»‘ tÆ°Æ¡ng quan vá»›i Visim-400**
fSimlex = "word/Visim-400.txt"
word_pairs = []
human_scores = []  # Dá»¯ liá»‡u con ngÆ°á»i
model_scores = []  # Dá»¯ liá»‡u tá»« thuáº­t toÃ¡n
w1='nao_nÃºng'
w2='kiÃªn_Ä‘á»‹nh'

with open(fSimlex, "r", encoding="utf-8") as f:
    for line in f.readlines()[1:]:  # Bá» qua tiÃªu Ä‘á»
        parts = line.strip().split()
        if len(parts) >= 4:
            w1, w2, pos, sim1 = parts[0], parts[1], parts[2], float(parts[3])
            word_pairs.append((w1, w2, pos, sim1))

# ğŸ”„ Cháº¡y trÃªn tá»«ng cáº·p tá»« trong Visim-400
for w1, w2, pos, sim1 in word_pairs:
    sim_score = find_similarity(w1, w2)
    if sim_score is not None :
        human_scores.append(sim1 / 6)  # Chuáº©n hÃ³a vá» [0,1]
        model_scores.append(sim_score)
Max=max(model_scores)
Min=min(model_scores)
for i in range(len(model_scores)):
    model_scores[i]=1-(model_scores[i]-Min)/(Max-Min)
# ğŸ“Š **TÃ­nh há»‡ sá»‘ tÆ°Æ¡ng quan Pearson**
if len(model_scores) > 1:
    pearson_corr, p_value = pearsonr(model_scores, human_scores)
    print(f"\nğŸ“Š Há»‡ sá»‘ tÆ°Æ¡ng quan Pearson: {pearson_corr:.4f}")
    print(f"ğŸ“Œ P-value: {p_value:.4f}")
else:
    print("âŒ KhÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ tÃ­nh há»‡ sá»‘ tÆ°Æ¡ng quan.")
